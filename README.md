# Service Weaver Analysis Dataset

**Dataset de Arqueologia Digital: AnÃ¡lise da DescontinuaÃ§Ã£o do Service Weaver**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## Sobre o Dataset

Este dataset documenta a trajetÃ³ria completa do **Service Weaver**, um framework Go para desenvolvimento de aplicaÃ§Ãµes modulares criado pelo Google, desde seu lanÃ§amento em marÃ§o de 2023 atÃ© sua descontinuaÃ§Ã£o em dezembro de 2024.

### Objetivo

Fornecer um **dataset estruturado e reproduzÃ­vel** para pesquisas em:
- **Arqueologia de Software**: AnÃ¡lise post-mortem de projetos descontinuados
- **Engenharia de Software EmpÃ­rica**: Estudos sobre adoÃ§Ã£o, manutenÃ§Ã£o e declÃ­nio de frameworks
- **MineraÃ§Ã£o de RepositÃ³rios**: AnÃ¡lise de discussÃµes tÃ©cnicas em mÃºltiplas plataformas
- **AnÃ¡lise de Sentimento**: PercepÃ§Ã£o da comunidade sobre decisÃµes tÃ©cnicas

---

## ComposiÃ§Ã£o do Dataset

### Fontes de Dados Coletadas

O dataset agrega **1.035+ registros** de 5 fontes diferentes:

| Fonte | Tipos de Dados | Volume | PerÃ­odo Coberto |
|-------|---------------|--------|-----------------|
| **GitHub** | Issues, Pull Requests, ComentÃ¡rios | ~750+ registros | Mar/2023 - Mai/2025 |
| **Hacker News** | Stories, ComentÃ¡rios | ~250+ registros | 2023 - 2025 |
| **Reddit** | Posts, ComentÃ¡rios | ~20+ registros | 2009 - 2024 |
| **Stack Overflow** | Perguntas, Respostas | 0* (API limitada) | - |
| **Google Groups** | Threads, Mensagens | 0* (robots.txt) | - |

*\*Fontes bloqueadas ou com restriÃ§Ãµes de acesso*

### Estrutura dos Dados

Cada registro contÃ©m:
- **`source`**: Origem do dado (github_issue, github_pr, reddit, hackernews_story, etc.)
- **`data_id`**: Identificador Ãºnico (ex: gh_issue_816, reddit_11hkh43)
- **`timestamp`**: Data/hora de criaÃ§Ã£o no formato ISO 8601
- **`raw_text`**: ConteÃºdo textual completo (tÃ­tulo + corpo + comentÃ¡rios)
- **`author_id`**: Hash anonimizado do autor (SHA-256)
- **`url`**: Link direto para o recurso original
- **`metadata`**: JSON com informaÃ§Ãµes especÃ­ficas da plataforma

### Tags Aplicadas (Camadas de CodificaÃ§Ã£o)

#### **Temporal Period** (PeriodizaÃ§Ã£o HistÃ³rica)
Classifica registros em 5 fases do ciclo de vida:

- **`pre_launch`** (< Mar/2023): DiscussÃµes anteriores ao lanÃ§amento
- **`early_adoption`** (Mar-Jun/2023): Fase inicial de adoÃ§Ã£o
- **`plateau`** (Jun/2023-Jun/2024): Maturidade e estabilizaÃ§Ã£o
- **`decline`** (Jun-Dez/2024): Sinais de declÃ­nio
- **`post_discontinuation`** (> Dez/2024): ApÃ³s anÃºncio oficial

#### **Resolution Status** (Status de ResoluÃ§Ã£o)
Indica o desfecho de issues/discussÃµes:

- **`unknown`**: Status nÃ£o determinado (85% dos casos)
- **`acknowledged_not_fixed`**: Problema reconhecido mas nÃ£o resolvido
- **`fixed`**: Resolvido
- **`abandoned`**: Abandonado sem resoluÃ§Ã£o
- **`wontfix`**: Marcado como "nÃ£o serÃ¡ corrigido"

#### **Root Cause Category** (Categoria de Causa Raiz)
Classifica problemas/discussÃµes por origem:

- **`unclear`**: Causa nÃ£o identificada
- **`architectural_limitation`**: LimitaÃ§Ãµes arquiteturais
- **`community_mismatch`**: Desalinhamento com necessidades da comunidade
- **`technical_debt`**: DÃ­vida tÃ©cnica acumulada
- **`resource_constraint`**: Falta de recursos (tempo, pessoas, orÃ§amento)

---

## Metodologia de Coleta

### Pipeline Automatizado

```
1. COLETA â†’ 2. DEDUPLICAÃ‡ÃƒO â†’ 3. ANONIMIZAÃ‡ÃƒO â†’ 4. TAGGING â†’ 5. ANÃLISE
```

1. **Coleta Multi-Fonte**: Scrapers Python com rate limiting e retry automÃ¡tico
2. **DeduplicaÃ§Ã£o**: RemoÃ§Ã£o de duplicatas por hash de URL
3. **AnonimizaÃ§Ã£o**: Hash SHA-256 de identificadores de autores
4. **Tagging**: Sistema de tags em 3 camadas (temporal, resoluÃ§Ã£o, causa raiz)
5. **AnÃ¡lise**: EstatÃ­sticas descritivas + visualizaÃ§Ãµes

### ConfiguraÃ§Ã£o TÃ©cnica

- **Rate Limiting**: 1 segundo entre requisiÃ§Ãµes (GitHub), variÃ¡vel por API
- **Retry Policy**: 3 tentativas com backoff exponencial
- **PerÃ­odo**: Mar/2023 - Dez/2024 (com alguns registros fora do perÃ­odo)
- **Credenciais**: GitHub Token, Reddit API (configurÃ¡veis via `.env`)

---

## Como Usar Este Dataset

### InstalaÃ§Ã£o RÃ¡pida

```powershell
# 1. Clone o repositÃ³rio
git clone <seu-repo-url>
cd service_weaver_analysis

# 2. Instale dependÃªncias
pip install -r requirements.txt

# 3. Configure credenciais (opcional para re-coletar)
copy .env.example .env
notepad .env  # Adicione suas API keys
```

### Uso BÃ¡sico: Carregar Dataset Processado

```python
import pandas as pd

# Carregar dataset taggeado (pronto para anÃ¡lise)
df = pd.read_csv('3_processed_data/tagged_dataset.csv')

# Explorar estrutura
print(df.columns)
print(df['temporal_period'].value_counts())
print(df.groupby(['source', 'temporal_period']).size())
```

### Re-executar Coleta (Atualizar Dataset)

```powershell
# Coletar novos dados
python -m 1_data_collection.scrapers.collect_all

# Combinar e taggear
python -c "
import pandas as pd, glob
dfs = [pd.read_csv(f) for f in glob.glob('1_data_collection/raw_datasets/*_raw_*.csv')]
pd.concat(dfs).to_csv('3_processed_data/combined_raw_data.csv', index=False)
"

python 2_coding_methodology/tag_layering.py 3_processed_data/combined_raw_data.csv
```

### AnÃ¡lise e VisualizaÃ§Ã£o

```powershell
# Gerar anÃ¡lise estatÃ­stica
python -c "exec(open('scripts/analyze.py').read())"

# Gerar visualizaÃ§Ãµes
python -c "exec(open('scripts/visualize.py').read())"
```

---

## Adaptando para Outros Projetos

### Passo 1: Configurar Novo Alvo

Edite `config.yaml`:

```yaml
project_name: seu_projeto_aqui

sources:
  github:
    repository: Organization/repository-name  # Mude aqui
    
  reddit:
    subreddits: [subreddit1, subreddit2]
    keywords: [keyword1, keyword2]
    
  hackernews:
    keywords: [projeto_nome]
```

### Passo 2: Ajustar PerÃ­odos Temporais

```yaml
date_range:
  start: '2020-01-01'  # Data de lanÃ§amento do projeto
  end: '2024-12-31'    # Data de anÃ¡lise/descontinuaÃ§Ã£o

tagging:
  temporal_periods:
    pre_launch: '2020-01-01'
    early_adoption: '2020-06-30'
    plateau: '2023-01-01'
    decline: '2024-06-30'
```

### Passo 3: Re-coletar e Processar

```powershell
python -m 1_data_collection.scrapers.collect_all
python 2_coding_methodology/tag_layering.py 3_processed_data/combined_raw_data.csv
```

### Passo 4: CodificaÃ§Ã£o Manual (Opcional)

Para anÃ¡lise aprofundada, codifique manualmente uma amostra:

```powershell
# Ferramenta auxiliar de double-coding
python 2_coding_methodology/double_code_tool.py --help
```

---

## ğŸ“‚ Estrutura de Arquivos

---

## ğŸ“‚ Estrutura do Projeto

```
service_weaver_analysis/
â”œâ”€â”€ 1_data_collection/          # Coleta de dados
â”‚   â”œâ”€â”€ scrapers/               # Scripts de scraping
â”‚   â”‚   â”œâ”€â”€ collect_all.py      # Orquestrador principal
â”‚   â”‚   â”œâ”€â”€ github_scraper.py
â”‚   â”‚   â”œâ”€â”€ reddit_scraper.py
â”‚   â”‚   â”œâ”€â”€ stackoverflow_scraper.py
â”‚   â”‚   â”œâ”€â”€ hackernews_scraper.py
â”‚   â”‚   â””â”€â”€ google_groups_scraper.py
â”‚   â””â”€â”€ raw_datasets/           # Dados brutos (CSV)
â”‚
â”œâ”€â”€ 2_coding_methodology/       # CodificaÃ§Ã£o temÃ¡tica
â”‚   â”œâ”€â”€ coding_scheme.json      # Esquema de categorias
â”‚   â”œâ”€â”€ codebook.md             # Manual de codificaÃ§Ã£o
â”‚   â”œâ”€â”€ double_code_tool.py     # Ferramenta auxiliar
â”‚   â””â”€â”€ tag_layering.py         # Sistema de tags
â”‚
â”œâ”€â”€ 3_processed_data/           # Dados processados
â”‚   â””â”€â”€ tagged_data.csv         # Dataset final taggeado
â”‚
â”œâ”€â”€ 4_analysis/                 # AnÃ¡lises e visualizaÃ§Ãµes
â”‚   â”œâ”€â”€ statistical_analysis.py
â”‚   â”œâ”€â”€ generate_visualizations.py
â”‚   â””â”€â”€ visualizations/         # GrÃ¡ficos gerados
â”‚
â”œâ”€â”€ tests/                      # Testes unitÃ¡rios
â”œâ”€â”€ config.yaml                 # ConfiguraÃ§Ã£o
â””â”€â”€ requirements.txt            # DependÃªncias
```

---

## ğŸ”¬ Metodologia

### 1. Coleta de Dados
- **Fontes**: GitHub, Stack Overflow, Reddit, Hacker News, Google Groups
- **PerÃ­odo**: Mar/2023 (lanÃ§amento) - Dez/2024 (descontinuaÃ§Ã£o)
- **Output**: CSVs em `raw_datasets/`

### 2. CodificaÃ§Ã£o TemÃ¡tica
- Esquema hierÃ¡rquico de categorias (6 principais, 24 sub-cÃ³digos)
- Ferramenta de double-coding para validaÃ§Ã£o
- Output: Dados codificados em `3_processed_data/`

### 3. Tag Layering
- **Temporal**: pre_launch, early_adoption, plateau, decline, post_discontinuation
- **ResoluÃ§Ã£o**: fixed, abandoned, wontfix, acknowledged_not_fixed
- **Root Cause**: architectural_limitation, community_mismatch, technical_debt, etc.

### 4. AnÃ¡lise & VisualizaÃ§Ã£o
- AnÃ¡lises estatÃ­sticas (Ï‡Â², correlaÃ§Ãµes)
- VisualizaÃ§Ãµes interativas (timelines, heatmaps)
- Output: GrÃ¡ficos em `4_analysis/visualizations/`

---

## âš™ï¸ ConfiguraÃ§Ã£o

Edite `config.yaml` para personalizar:

```yaml
sources:
  github:
    enabled: true
    max_items: 5000
  
  reddit:
    enabled: true
    subreddits: ["golang", "microservices"]
```

---

## ğŸ§ª Testes

```powershell
pytest tests/ -v
```

---

## ï¿½ Dados

- **Dataset bruto**: `1_data_collection/raw_datasets/*.csv`
- **Dataset processado**: `3_processed_data/tagged_data.csv`
- **VisualizaÃ§Ãµes**: `4_analysis/visualizations/`

---

## ğŸ“§ Contato

Para questÃµes, abra uma issue no GitHub.

---

**Status**: ğŸš§ Em Desenvolvimento

**Atualizado**: Outubro 2025
